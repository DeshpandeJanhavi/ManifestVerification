{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two layer CNN is run on small dataset consisting of 20 images, 5 images per category. Input has two channels, log image and original image. Network did not overfit on this data and the loss is constant even after 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1] = 0.25\n",
      "[1] running_loss: 4284005839002123264.000 loss: 825274528011046400.000\n",
      "Accuracy: [11] = 0.25\n",
      "[11] running_loss: 1.397 loss: 1.405\n",
      "Accuracy: [21] = 0.25\n",
      "[21] running_loss: 1.387 loss: 1.380\n",
      "Accuracy: [31] = 0.25\n",
      "[31] running_loss: 1.387 loss: 1.386\n",
      "Accuracy: [41] = 0.25\n",
      "[41] running_loss: 1.387 loss: 1.398\n",
      "Accuracy: [51] = 0.25\n",
      "[51] running_loss: 1.386 loss: 1.381\n",
      "Accuracy: [61] = 0.25\n",
      "[61] running_loss: 1.387 loss: 1.389\n",
      "Accuracy: [71] = 0.25\n",
      "[71] running_loss: 1.387 loss: 1.384\n",
      "Accuracy: [81] = 0.25\n",
      "[81] running_loss: 1.387 loss: 1.381\n",
      "Accuracy: [91] = 0.25\n",
      "[91] running_loss: 1.387 loss: 1.385\n",
      "Accuracy: [101] = 0.25\n",
      "[101] running_loss: 1.387 loss: 1.385\n",
      "Accuracy: [111] = 0.25\n",
      "[111] running_loss: 1.386 loss: 1.386\n",
      "Accuracy: [121] = 0.25\n",
      "[121] running_loss: 1.386 loss: 1.386\n",
      "Accuracy: [131] = 0.25\n",
      "[131] running_loss: 1.386 loss: 1.386\n",
      "Accuracy: [141] = 0.25\n",
      "[141] running_loss: 1.386 loss: 1.387\n",
      "Accuracy: [151] = 0.25\n",
      "[151] running_loss: 1.387 loss: 1.387\n",
      "Accuracy: [161] = 0.25\n",
      "[161] running_loss: 1.386 loss: 1.388\n",
      "Accuracy: [171] = 0.25\n",
      "[171] running_loss: 1.387 loss: 1.386\n",
      "Accuracy: [181] = 0.25\n",
      "[181] running_loss: 1.386 loss: 1.388\n",
      "Accuracy: [191] = 0.25\n",
      "[191] running_loss: 1.386 loss: 1.387\n",
      "Finished Training\n",
      "Accuracy of Apples :  0 %\n",
      "Accuracy of Dates :  0 %\n",
      "Accuracy of  TYRE : 100 %\n",
      "Accuracy of Paper :  0 %\n",
      "Total accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import csv\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_path, transform):\n",
    "        'Initialization'\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        X_apples = []\n",
    "        X_dates = []\n",
    "        X_tyre = []\n",
    "        X_paper = []\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "\n",
    "        for image_path in glob.glob(self.data_path + 'Apples/img*/csv/log/*.csv'):\n",
    "            log = np.array(list(csv.reader(open(image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            original_image_path = image_path.replace(\"log\",\"original\")\n",
    "            original = np.array(list(csv.reader(open(original_image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            channels = []\n",
    "            channels.append(log)\n",
    "            channels.append(original)\n",
    "            \n",
    "            channels = np.asarray(channels,\"double\")\n",
    "            X_apples.append(channels)\n",
    "            \n",
    "        X_apples = np.asarray(X_apples, \"double\")\n",
    "        \n",
    "        # #assign labels\n",
    "        Y_apples = np.zeros(X_apples.shape[0], dtype=np.long)\n",
    "\n",
    "        for image_path in glob.glob(self.data_path + 'Dates/img*/csv/log/*.csv'):\n",
    "            log = np.array(list(csv.reader(open(image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            original_image_path = image_path.replace(\"log\",\"original\")\n",
    "            original = np.array(list(csv.reader(open(original_image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            channels = []\n",
    "            channels.append(log)\n",
    "            channels.append(original)\n",
    "            \n",
    "            channels = np.asarray(channels,\"double\")\n",
    "            X_dates.append(channels)\n",
    "            \n",
    "        X_dates = np.asarray(X_dates, \"double\")\n",
    "        \n",
    "        # #assign labels\n",
    "        Y_dates = np.ones(X_dates.shape[0], dtype=np.long)\n",
    "\n",
    "        for image_path in glob.glob(self.data_path + 'TYRE/img*/csv/log/*.csv'):\n",
    "            log = np.array(list(csv.reader(open(image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            original_image_path = image_path.replace(\"log\",\"original\")\n",
    "            original = np.array(list(csv.reader(open(original_image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            channels = []\n",
    "            channels.append(log)\n",
    "            channels.append(original)\n",
    "           \n",
    "            channels = np.asarray(channels,\"double\")\n",
    "            X_tyre.append(channels)\n",
    "           \n",
    "        X_tyre = np.asarray(X_tyre, \"double\")\n",
    "       \n",
    "        # #assign labels\n",
    "        Y_tyre = 2 * np.ones(X_tyre.shape[0], dtype=np.long)\n",
    "\n",
    "        for image_path in glob.glob(self.data_path + 'Paper/img*/csv/log/*.csv'):\n",
    "            log = np.array(list(csv.reader(open(image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            original_image_path = image_path.replace(\"log\",\"original\")\n",
    "            original = np.array(list(csv.reader(open(original_image_path, \"r\"), delimiter=\",\"))).astype(\"double\")\n",
    "            channels = []\n",
    "            channels.append(log)\n",
    "            channels.append(original)\n",
    "            \n",
    "            channels = np.asarray(channels,\"double\")\n",
    "            X_paper.append(channels)\n",
    "           \n",
    "        X_paper = np.asarray(X_paper, \"double\")\n",
    "        \n",
    "        # #assign labels\n",
    "        Y_paper = 3 * np.ones(X_paper.shape[0], dtype=np.long)\n",
    "\n",
    "        self.X = np.append(X_apples, X_dates, axis=0)\n",
    "        self.X = np.append(self.X,X_tyre, axis=0)\n",
    "        self.X = np.append(self.X,X_paper, axis=0)\n",
    "        self.Y = np.append(Y_apples,Y_dates, axis=0)\n",
    "        self.Y = np.append(self.Y,Y_tyre, axis=0)\n",
    "        self.Y = np.append(self.Y,Y_paper, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        image = self.X[index]\n",
    "        image = image.transpose((1, 2, 0))\n",
    "        label = self.Y[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "training_set = Dataset('./data/train/', transform=transform)\n",
    "training_generator = data.DataLoader(training_set, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validation_set = Dataset('./data/test/', transform=transform)\n",
    "validation_generator = data.DataLoader(validation_set, batch_size=4,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ( 'Apples', 'Dates','TYRE', 'Paper')\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 512)\n",
    "        #self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 13 * 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "n_mini_batch = 0\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    class_correct_train = list(0. for i in range(4))\n",
    "    class_total_train = list(0. for i in range(4))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, labelled_data in enumerate(training_generator, 0):\n",
    "        n_mini_batch += 1\n",
    "        # get the inputs\n",
    "        inputs, labels = labelled_data\n",
    "        inputs = inputs.type(torch.DoubleTensor)\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct_train[label] += c[i].item()\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Accuracy: [\" + str(epoch + 1) + \"] = \" + str(np.sum(class_correct_train) / np.sum(class_total_train)))\n",
    "        print('[%d] running_loss: %.3f loss: %.3f' % (epoch + 1, running_loss/5, loss.item()))\n",
    "        running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "with torch.no_grad():\n",
    "    for labelled_data in validation_generator:\n",
    "        images, labels = labelled_data\n",
    "        images = images.type(torch.DoubleTensor)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print (\"Total accuracy: \" + str(np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the learning rate by factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1] = 0.15\n",
      "[1] running_loss: 111803715180085730857180037398921216.000 loss: 559015822954375394181857009609998336.000\n",
      "Accuracy: [11] = 0.25\n",
      "[11] running_loss: 1.390 loss: 1.389\n",
      "Accuracy: [21] = 0.1\n",
      "[21] running_loss: 1.388 loss: 1.384\n",
      "Accuracy: [31] = 0.25\n",
      "[31] running_loss: 1.388 loss: 1.389\n",
      "Accuracy: [41] = 0.15\n",
      "[41] running_loss: 1.387 loss: 1.387\n",
      "Accuracy: [51] = 0.25\n",
      "[51] running_loss: 1.388 loss: 1.391\n",
      "Accuracy: [61] = 0.25\n",
      "[61] running_loss: 1.388 loss: 1.392\n",
      "Accuracy: [71] = 0.25\n",
      "[71] running_loss: 1.389 loss: 1.389\n",
      "Accuracy: [81] = 0.25\n",
      "[81] running_loss: 1.387 loss: 1.386\n",
      "Accuracy: [91] = 0.25\n",
      "[91] running_loss: 1.388 loss: 1.392\n",
      "Accuracy: [101] = 0.25\n",
      "[101] running_loss: 1.387 loss: 1.386\n",
      "Accuracy: [111] = 0.1\n",
      "[111] running_loss: 1.387 loss: 1.389\n",
      "Accuracy: [121] = 0.25\n",
      "[121] running_loss: 1.388 loss: 1.388\n",
      "Accuracy: [131] = 0.2\n",
      "[131] running_loss: 1.388 loss: 1.393\n",
      "Accuracy: [141] = 0.2\n",
      "[141] running_loss: 1.389 loss: 1.395\n",
      "Accuracy: [151] = 0.25\n",
      "[151] running_loss: 1.388 loss: 1.390\n",
      "Accuracy: [161] = 0.25\n",
      "[161] running_loss: 1.387 loss: 1.388\n",
      "Accuracy: [171] = 0.25\n",
      "[171] running_loss: 1.388 loss: 1.398\n",
      "Accuracy: [181] = 0.25\n",
      "[181] running_loss: 1.387 loss: 1.388\n",
      "Accuracy: [191] = 0.2\n",
      "[191] running_loss: 1.388 loss: 1.386\n",
      "Finished Training\n",
      "Accuracy of Apples :  0 %\n",
      "Accuracy of Dates :  0 %\n",
      "Accuracy of  TYRE : 100 %\n",
      "Accuracy of Paper :  0 %\n",
      "Total accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    class_correct_train = list(0. for i in range(4))\n",
    "    class_total_train = list(0. for i in range(4))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, labelled_data in enumerate(training_generator, 0):\n",
    "        n_mini_batch += 1\n",
    "        # get the inputs\n",
    "        inputs, labels = labelled_data\n",
    "        inputs = inputs.type(torch.DoubleTensor)\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct_train[label] += c[i].item()\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Accuracy: [\" + str(epoch + 1) + \"] = \" + str(np.sum(class_correct_train) / np.sum(class_total_train)))\n",
    "        print('[%d] running_loss: %.3f loss: %.3f' % (epoch + 1, running_loss/5, loss.item()))\n",
    "        running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "with torch.no_grad():\n",
    "    for labelled_data in validation_generator:\n",
    "        images, labels = labelled_data\n",
    "        images = images.type(torch.DoubleTensor)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print (\"Total accuracy: \" + str(np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further increasing learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1] = 0.25\n",
      "[1] running_loss: 330823187869420467681708629774177073362824786345984.000 loss: 4493120137215466281146504801817063013783126933504.000\n",
      "Accuracy: [11] = 0.2\n",
      "[11] running_loss: 1.409 loss: 1.436\n",
      "Accuracy: [21] = 0.1\n",
      "[21] running_loss: 1.405 loss: 1.425\n",
      "Accuracy: [31] = 0.05\n",
      "[31] running_loss: 1.408 loss: 1.390\n",
      "Accuracy: [41] = 0.2\n",
      "[41] running_loss: 1.393 loss: 1.426\n",
      "Accuracy: [51] = 0.1\n",
      "[51] running_loss: 1.399 loss: 1.439\n",
      "Accuracy: [61] = 0.25\n",
      "[61] running_loss: 1.390 loss: 1.403\n",
      "Accuracy: [71] = 0.2\n",
      "[71] running_loss: 1.396 loss: 1.408\n",
      "Accuracy: [81] = 0.25\n",
      "[81] running_loss: 1.395 loss: 1.436\n",
      "Accuracy: [91] = 0.15\n",
      "[91] running_loss: 1.407 loss: 1.430\n",
      "Accuracy: [101] = 0.2\n",
      "[101] running_loss: 1.396 loss: 1.445\n",
      "Accuracy: [111] = 0.15\n",
      "[111] running_loss: 1.410 loss: 1.464\n",
      "Accuracy: [121] = 0.25\n",
      "[121] running_loss: 1.417 loss: 1.549\n",
      "Accuracy: [131] = 0.05\n",
      "[131] running_loss: 1.418 loss: 1.459\n",
      "Accuracy: [141] = 0.15\n",
      "[141] running_loss: 1.421 loss: 1.503\n",
      "Accuracy: [151] = 0.1\n",
      "[151] running_loss: 1.397 loss: 1.387\n",
      "Accuracy: [161] = 0.15\n",
      "[161] running_loss: 1.405 loss: 1.420\n",
      "Accuracy: [171] = 0.05\n",
      "[171] running_loss: 1.411 loss: 1.430\n",
      "Accuracy: [181] = 0.2\n",
      "[181] running_loss: 1.402 loss: 1.404\n",
      "Accuracy: [191] = 0.25\n",
      "[191] running_loss: 1.395 loss: 1.393\n",
      "Finished Training\n",
      "Accuracy of Apples : 100 %\n",
      "Accuracy of Dates :  0 %\n",
      "Accuracy of  TYRE :  0 %\n",
      "Accuracy of Paper :  0 %\n",
      "Total accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    class_correct_train = list(0. for i in range(4))\n",
    "    class_total_train = list(0. for i in range(4))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, labelled_data in enumerate(training_generator, 0):\n",
    "        n_mini_batch += 1\n",
    "        # get the inputs\n",
    "        inputs, labels = labelled_data\n",
    "        inputs = inputs.type(torch.DoubleTensor)\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct_train[label] += c[i].item()\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Accuracy: [\" + str(epoch + 1) + \"] = \" + str(np.sum(class_correct_train) / np.sum(class_total_train)))\n",
    "        print('[%d] running_loss: %.3f loss: %.3f' % (epoch + 1, running_loss/5, loss.item()))\n",
    "        running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "with torch.no_grad():\n",
    "    for labelled_data in validation_generator:\n",
    "        images, labels = labelled_data\n",
    "        images = images.type(torch.DoubleTensor)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print (\"Total accuracy: \" + str(np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1] = 0.15\n",
      "[1] running_loss: 2029914068329814528.000 loss: 12843498.249\n",
      "Accuracy: [11] = 0.25\n",
      "[11] running_loss: 1.609 loss: 1.889\n",
      "Accuracy: [21] = 0.25\n",
      "[21] running_loss: 1.530 loss: 1.251\n",
      "Accuracy: [31] = 0.25\n",
      "[31] running_loss: 1.545 loss: 1.727\n",
      "Accuracy: [41] = 0.2\n",
      "[41] running_loss: 1.606 loss: 1.791\n",
      "Accuracy: [51] = 0.0\n",
      "[51] running_loss: 1.857 loss: 1.901\n",
      "Accuracy: [61] = 0.4\n",
      "[61] running_loss: 1.816 loss: 1.885\n",
      "Accuracy: [71] = 0.15\n",
      "[71] running_loss: 1.813 loss: 2.129\n",
      "Accuracy: [81] = 0.15\n",
      "[81] running_loss: 2.179 loss: 2.309\n",
      "Accuracy: [91] = 0.0\n",
      "[91] running_loss: 1.855 loss: 2.068\n",
      "Accuracy: [101] = 0.2\n",
      "[101] running_loss: 1.489 loss: 1.487\n",
      "Accuracy: [111] = 0.25\n",
      "[111] running_loss: 1.549 loss: 1.950\n",
      "Accuracy: [121] = 0.25\n",
      "[121] running_loss: 1.482 loss: 1.382\n",
      "Accuracy: [131] = 0.1\n",
      "[131] running_loss: 1.637 loss: 1.740\n",
      "Accuracy: [141] = 0.3\n",
      "[141] running_loss: 1.573 loss: 2.202\n",
      "Accuracy: [151] = 0.15\n",
      "[151] running_loss: 1.908 loss: 1.770\n",
      "Accuracy: [161] = 0.15\n",
      "[161] running_loss: 1.484 loss: 1.575\n",
      "Accuracy: [171] = 0.3\n",
      "[171] running_loss: 2.085 loss: 2.560\n",
      "Accuracy: [181] = 0.25\n",
      "[181] running_loss: 1.599 loss: 1.588\n",
      "Accuracy: [191] = 0.3\n",
      "[191] running_loss: 1.655 loss: 1.947\n",
      "Finished Training\n",
      "Accuracy of Apples :  0 %\n",
      "Accuracy of Dates : 100 %\n",
      "Accuracy of  TYRE :  0 %\n",
      "Accuracy of Paper :  0 %\n",
      "Total accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=1, momentum=0.9)\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    class_correct_train = list(0. for i in range(4))\n",
    "    class_total_train = list(0. for i in range(4))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, labelled_data in enumerate(training_generator, 0):\n",
    "        n_mini_batch += 1\n",
    "        # get the inputs\n",
    "        inputs, labels = labelled_data\n",
    "        inputs = inputs.type(torch.DoubleTensor)\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct_train[label] += c[i].item()\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Accuracy: [\" + str(epoch + 1) + \"] = \" + str(np.sum(class_correct_train) / np.sum(class_total_train)))\n",
    "        print('[%d] running_loss: %.3f loss: %.3f' % (epoch + 1, running_loss/5, loss.item()))\n",
    "        running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "with torch.no_grad():\n",
    "    for labelled_data in validation_generator:\n",
    "        images, labels = labelled_data\n",
    "        images = images.type(torch.DoubleTensor)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print (\"Total accuracy: \" + str(np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normalized input_channels, network is overfitting on small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1] = 0.35\n",
      "[1] running_loss: 1.421 loss: 1.295\n",
      "Accuracy: [11] = 0.95\n",
      "[11] running_loss: 0.266 loss: 0.358\n",
      "Accuracy: [21] = 1.0\n",
      "[21] running_loss: 0.043 loss: 0.003\n",
      "Accuracy: [31] = 1.0\n",
      "[31] running_loss: 0.009 loss: 0.014\n",
      "Accuracy: [41] = 1.0\n",
      "[41] running_loss: 0.005 loss: 0.000\n",
      "Accuracy: [51] = 1.0\n",
      "[51] running_loss: 0.003 loss: 0.004\n",
      "Accuracy: [61] = 1.0\n",
      "[61] running_loss: 0.002 loss: 0.000\n",
      "Accuracy: [71] = 1.0\n",
      "[71] running_loss: 0.002 loss: 0.005\n",
      "Accuracy: [81] = 1.0\n",
      "[81] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [91] = 1.0\n",
      "[91] running_loss: 0.001 loss: 0.000\n",
      "Accuracy: [101] = 1.0\n",
      "[101] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [111] = 1.0\n",
      "[111] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [121] = 1.0\n",
      "[121] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [131] = 1.0\n",
      "[131] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [141] = 1.0\n",
      "[141] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [151] = 1.0\n",
      "[151] running_loss: 0.001 loss: 0.000\n",
      "Accuracy: [161] = 1.0\n",
      "[161] running_loss: 0.001 loss: 0.001\n",
      "Accuracy: [171] = 1.0\n",
      "[171] running_loss: 0.001 loss: 0.000\n",
      "Accuracy: [181] = 1.0\n",
      "[181] running_loss: 0.000 loss: 0.000\n",
      "Accuracy: [191] = 1.0\n",
      "[191] running_loss: 0.000 loss: 0.001\n",
      "Finished Training\n",
      "Accuracy of Apples : 80 %\n",
      "Accuracy of Dates : 80 %\n",
      "Accuracy of  TYRE : 20 %\n",
      "Accuracy of Paper :  0 %\n",
      "Total accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((7.78748281657, 7025.73150657), (0.854766446669, 7977.58896648))])\n",
    "\n",
    "training_set = Dataset('./data/train/', transform=transform)\n",
    "training_generator = data.DataLoader(training_set, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validation_set = Dataset('./data/test/', transform=transform)\n",
    "validation_generator = data.DataLoader(validation_set, batch_size=4,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    class_correct_train = list(0. for i in range(4))\n",
    "    class_total_train = list(0. for i in range(4))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, labelled_data in enumerate(training_generator, 0):\n",
    "        n_mini_batch += 1\n",
    "        # get the inputs\n",
    "        inputs, labels = labelled_data\n",
    "        inputs = inputs.type(torch.DoubleTensor)\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct_train[label] += c[i].item()\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Accuracy: [\" + str(epoch + 1) + \"] = \" + str(np.sum(class_correct_train) / np.sum(class_total_train)))\n",
    "        print('[%d] running_loss: %.3f loss: %.3f' % (epoch + 1, running_loss/5, loss.item()))\n",
    "        running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "with torch.no_grad():\n",
    "    for labelled_data in validation_generator:\n",
    "        images, labels = labelled_data\n",
    "        images = images.type(torch.DoubleTensor)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print (\"Total accuracy: \" + str(np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
